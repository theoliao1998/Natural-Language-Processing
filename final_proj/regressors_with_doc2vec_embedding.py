# based on code in 
# https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4

# -*- coding: utf-8 -*-
"""regressors_with_doc2vec_embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12ZLFRAkCsaMm7zwdCWae6H6gLih68_XA
"""

import pandas as pd
import numpy as np
from tqdm import tqdm
tqdm.pandas(desc="progress-bar")
from gensim.models import Doc2Vec
from sklearn import utils
from sklearn.model_selection import train_test_split
import gensim
from sklearn.linear_model import LogisticRegression
from gensim.models.doc2vec import TaggedDocument
import re
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/gdrive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My\ Drive/630final

import pandas as pd

train_df = pd.read_csv('./data/train.csv')
val_df = pd.read_csv('./data/dev.csv')
test_df = pd.read_csv('./data/test.csv')

train_df['text'] = train_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1],x['edit']) ,axis=1)
val_df['text'] = val_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1],x['edit']) ,axis=1)
test_df['text'] = test_df.apply(lambda x:x['original'].replace(x['original'][x['original'].find('<'):x['original'].find('>')+1],x['edit']) ,axis=1)

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
def tokenize_text(text):
    tokens = []
    for sent in nltk.sent_tokenize(text):
        for word in nltk.word_tokenize(sent):
            if len(word) < 2:
                continue
            tokens.append(word.lower())
    return tokens

train_tagged = train_df.apply(
    lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r['meanGrade']]), axis=1)
test_tagged= val_df.apply(
    lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r['meanGrade']]), axis=1)

import multiprocessing
cores = multiprocessing.cpu_count()

model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)
model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for epoch in range(30):
#     model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)
#     model_dbow.alpha -= 0.002
#     model_dbow.min_alpha = model_dbow.alpha

def vec_for_learning(model, tagged_docs):
    sents = tagged_docs.values
    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])
    return targets, regressors

y_train, X_train = vec_for_learning(model_dbow, train_tagged)
y_test, X_test = vec_for_learning(model_dbow, test_tagged)
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

for i in [1,10,100,1000]:
  clf = SVR(C=i)
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  yy_test = np.array(val_df['meanGrade'])
  print(i,mean_squared_error(yy_test, y_pred, squared=False))

import numpy as np
import pandas as pd
import seaborn as sns

x =  [1,10,100,1000]
y = [0.580406325502122,0.5849076945927966,0.584575247839623,0.5847227160678179]
df1 = pd.DataFrame(data=y, index=x)
df2 = pd.DataFrame(data = {'C': x, 'RMSE': y})

ax = sns.lineplot(x="C", y="RMSE", data=df2)    

ax.set_xscale('log')

y_train, X_train = vec_for_learning(model_dbow, train_tagged)
y_test, X_test = vec_for_learning(model_dbow, test_tagged)
from sklearn.kernel_ridge import KernelRidge
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import validation_curve

for i in [1,0.1,0.01,0.001]:
  clf = KernelRidge(alpha=i)
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  yy_test = np.array(val_df['meanGrade'])
  print(i,mean_squared_error(yy_test, y_pred, squared=False))

y_train, X_train = vec_for_learning(model_dbow, train_tagged)
y_test, X_test = vec_for_learning(model_dbow, test_tagged)
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

for i in range(1,10):
  clf = RandomForestRegressor(n_estimators=10,max_depth=i)
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  yy_test = np.array(val_df['meanGrade'])
  print(i,mean_squared_error(yy_test, y_pred, squared=False))

for i in range(10,90,10):
  clf = RandomForestRegressor(n_estimators=i,max_depth=2)
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  yy_test = np.array(val_df['meanGrade'])
  print(i,mean_squared_error(yy_test, y_pred, squared=False))

